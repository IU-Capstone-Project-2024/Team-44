{
    "questions": [
        Question(
            question="What is the number of identical layers in the encoder?",
            options=["1", "2", "6", "3"],
            correct_answers=["6"],
        ),
        Question(
            question="What is the first sub-layer in each layer of the encoder?",
            options=[
                "Position-wise fully connected feed-forward network",
                "Multi-head self-attention mechanism",
                "Layer normalization",
                "Residual connection",
            ],
            correct_answers=["Multi-head self-attention mechanism"],
        ),
        Question(
            question="What is the second sub-layer in each layer of the encoder?",
            options=[
                "Simple, position-wise fully connected feed-forward network",
                "Multi-head self-attention mechanism",
                "Layer normalization",
                "Residual connection",
            ],
            correct_answers=[
                "Simple, position-wise fully connected feed-forward network"
            ],
        ),
        Question(
            question="What is used to facilitate residual connections in the model?",
            options=[
                "Sub-layer itself",
                "Layer normalization",
                "Embedding layers",
                "All sub-layers and embedding layers",
            ],
            correct_answers=["All sub-layers and embedding layers"],
        ),
        Question(
            question="What is composed of a stack of N = 6 identical layers?",
            options=["Encoder", "Decoder", "Transformer", "Attention"],
            correct_answers=["Decoder"],
        ),
        Question(
            question="What performs multi-head attention over the output of the encoder stack?",
            options=[
                "Encoder",
                "Decoder",
                "Self-attention sub-layer",
                "Layer normalization",
            ],
            correct_answers=["Decoder"],
        ),
        Question(
            question="Why do we modify the self-attention sub-layer in the decoder stack?",
            options=[
                "To prevent positions from attending to subsequent positions",
                "To ensure that the predictions for position i can depend only on the known outputs at positions less than i",
                "To add residual connections around each of the sub-layers",
                "To employ layer normalization",
            ],
            correct_answers=[
                "To prevent positions from attending to subsequent positions"
            ],
        ),
    ]
}