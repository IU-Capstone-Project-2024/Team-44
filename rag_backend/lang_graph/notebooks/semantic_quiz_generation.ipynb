{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_small = \"\"\"\n",
    "\t\t\tEncoder: The encoder is composed of a stack of N = 6 identical layers. Each layer has two sub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, position- wise fully connected feed-forward network. We employ a residual connection [11] around each of the two sub-layers, followed by layer normalization [1]. That is, the output of each sub-layer is LayerNorm(x + Sublayer(x)), where Sublayer(x) is the function implemented by the sub-layer itself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding layers, produce outputs of dimension dmodel = 512.\n",
    "\t\t\tDecoder: The decoder is also composed of a stack of N = 6 identical layers. In addition to the two sub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head attention over the output of the encoder stack. Similar to the encoder, we employ residual connections around each of the sub-layers, followed by layer normalization. We also modify the self-attention sub-layer in the decoder stack to prevent positions from attending to subsequent positions. This masking, combined with fact that the output embeddings are offset by one position, ensures that the predictions for position i can depend only on the known outputs at positions less than i.\n",
    "\t\t\"\"\"\n",
    "\n",
    "text_giant = \"\"\"\n",
    "World\n",
    "Roshar is the native name for the planet on which The Stormlight Archive is set. It is also the name of the supercontinent on which the main events of the series take place. The planet was settled by the two Shards Honor and Cultivation, while their rival Odium exercises his influence on the planet. [46] People from Roshar are called Rosharans.[47] Roshar is the second planet from its sun and has three moons, each of which waxes and wanes separately from the others.[48] The world is regularly assaulted by Highstorms, storms characterized by a very violent storm front traveling from east to west (beginning at the Origin), followed by weaker rains. The lands in Shinovar, farthest west on the main continent of Roshar, are mostly protected from the Highstorms by the high peaks of the Misted Mountains. Most plants that grow in Shinovar, which resemble real-world plant life, cannot grow in other parts of Roshar. Highstorms come frequently and, though they do not appear to follow a simple pattern, storm wardens are able to accurately predict their schedule through complex mathematics. Flora and fauna have evolved to cope with this condition.[49]\n",
    "\n",
    "Nations and regions:\n",
    "During the Heraldic Epochs, Roshar was ruled by a coalition of ten nations known as the Silver Kingdoms. In the Era of Solitude, following the departure of the Heralds and the demise of the Orders of Knights Radiant, those kingdoms split into smaller ones, some of the more important being:\n",
    "Alethkar\n",
    "Jah Keved\n",
    "Herdaz\n",
    "Thaylenah\n",
    "Kharbranth and Natani cities in the Frostlands\n",
    "Numerous Makabaki countries, with Azir being most prominent\n",
    "Shinovar\n",
    "Rira and Iri\n",
    "\n",
    "Races:\n",
    "The Stormlight Archive features several different races, the majority of which are different ethnicities of human or partly human. Some of these races include:\n",
    "Thaylens: Renowned traders and merchants native to an island nation. They possess long eyebrows that can be styled to either droop or curve behind their ears.\n",
    "Alethi: Native to the nation of Alethkar, the Alethi are members of one of the four Vorin nations. They have a famed military heritage and are possessed of tan skin and dark hair.\n",
    "Veden: Native to the Vorin nation of Jah Keved, the Vedens are characterized by pale skin and black hair. Some have red hair, indicating Unkalaki ancestry.\n",
    "Natan: Native to the Vorin nation inhabiting New Natanan, the Natan often wear gloves and have faintly bluish skin.\n",
    "Unkalaki (Horneaters): A relatively rare race, the Horneaters are called thus by other races because the Unkalaki consider animal horns, shells, and claws to be a delicacy. They possess reddish hair and dark skin, and stand well over seven feet (2.1 m) tall. The Unkalaki homeland is in the mountains of Jah Keved. Their culture is very different from the other Vorin cultures.\n",
    "Parshendi (Singers/Listeners): A proud nonhuman race, living on Shattered Plains with a strong warrior culture. The Parshendi are viewed by many other races as savages because of their culture and past deeds. They have marbled red and white or red and black skin that forms patterns unique to each individual and an exoskeleton that acts as natural armor. They are at war with the Alethi during the novels' main timeline. They use spren to morph into many different forms, each with a unique function and set of abilities. These forms also change the appearance of the Parshendi who use them, for example taking warform makes them more physically able and grants them the mindset of a soldier. The workform allows them to be sturdier to perform physical labor. They also communicate through songs and rhythms in their heads. At the start of the series, many members of this species are found in a mentally limited form known as Parshmen, who are enslaved by various human groups.\n",
    "Shin: A race native to the region of Shinovar, Shin have white skin and lack epicanthic folds (unlike the other races). They stand shorter than most others, averaging five feet tall. They also have bigger and rounder eyes.\n",
    "Makabaki: Native to the nation of Azir and neighboring countries, Makabaki have dark skin and hair.\n",
    "Dysian Aimians: otherwise known as the Sleepless. A non-human race native to Aimia, but of otherworldly origin. They are made of many small creatures with exoskeletons called hordelings. There are 24 known Sleepless on Roshar.\n",
    "Siah Aimians: a non-human race also native to Aimia. They are characterized as having white-blue skin and shadows that point the wrong way. They can also change their bodies slightly, for example adding a tattoo mentally, or removing their sense of smell.\n",
    "\n",
    "Class structure:\n",
    "Much of The Way of Kings takes place within the nations of Alethkar and Jah Keved. Both of these nations divide their people into classes, primarily based on the color of their eyes. Those with dark eye colors (brown, dark green, charcoal grey) are mostly peasants (and can even be made slaves). Those with light eye colors (blue, yellow, tan, green, violet, orange, etc.) are the nobles and generally more educated ruling class. Within these classes, there are further class distinctions known as nahn (for darkeyes) and dahn (for lighteyes). Both have ten levels within. For the nahn, they range from slaves in the 10th nahn to full citizens with the right to travel in the second and first nahn. In the dahn system, lighteyes in the 10th dahn are considered only slightly better than darkeyes, and a very rich darkeyed man or woman may marry into an extremely poor lighteyed family, in very rare cases. The first dahn is composed of the king alone. It has been known for dark eyed individuals to obtain light eyes through obtaining Shardblades, a supernatural weapon in this world; however, this is exceptionally rare.\n",
    "\n",
    "Spren:\n",
    "Spren are spirits in the land of Roshar which are drawn to different conditions or emotions. There are thousands of varieties. One character, Hesina, the mother of Kaladin states, \"Spren appear when something changes - when fear appears, or when it begins to rain. They are the heart of change, and therefore the heart of all things.\"[50] Their intelligence varies, with Cryptics (also known as liespren though they themselves dislike the term) and honorspren among the most intelligent, and more common spren, seen as forces of nature/emotion having little to no intelligence. Jasnah Kholin also mentions that the 10 orders of the Knights Radiant drew their power from spren. Some notable spren are Syl, an Honorspren who shares a bond with Kaladin, giving him surgebinding powers of Windrunner; Pattern, a Cryptic who created a bond with Shallan, allowing her to surgebind; and the cultivationspren Wyndle, who bonded with the thief Lift, allowing her to surgebind. Dalinar Kholin also bonds a spren, the Stormfather, though he does so in an unconventional manner. Jasnah bonded an inkspren named Ivory.\n",
    "Some spren, such as flamespren, share characteristics with current observations in quantum mechanics, and are based on them.[51] For example, when they are observed they remain stable in the recorded state, but when tested more thoroughly, they change as though at random.\n",
    "As revealed in the second book, Spren are \"concepts and ideas\" given physical form by the human collective subconscious. Among the many forms of spren, some are intelligent, possess self-awareness, and have even built their own cities. They reside naturally in Shadesmar, and often cross over into the physical realm. This comes at the cost of most of their self-awareness for the higher, more exalted spren, which they can regain by making bonds with humans. The sea and land are reversed in Shadesmar—what would be land on Roshar is a sea of black beads in Shadesmar, each representing a physical form on Roshar. Shadesmar also contains cities and a strange type of flora.\n",
    "\n",
    "Religion:\n",
    "Much of the world follows the Vorin religion. Vorinism tells of a struggle between forces of the Voidbringers and humanity. The Voidbringers forced humanity out of its afterlife, called the Tranquiline Halls. They believe that upon death the soul continues in its past role, but towards the regaining of the Tranquiline Halls. In Alethkar, a man's highest calling is as a warrior in life to remain a warrior in the afterlife. The religion also tells of the Lost Radiants, an order who once fought against the Voidbringers during the wars against them on Roshar (known as Desolations). Vorinism gave the Knights Radiant the moniker \"Lost Radiants\" after they apparently betrayed humanity at some point in the distant past. Vorinism is arranged in devotaries, whose ardents aim to assist people in advancing their Callings, which are tasks to which one dedicates their life as a method of worship. Each person selects a devotary based on variances in beliefs, talents or personality traits, and may change their selection at any point in their life. Some examples are the Devotary of Sincerity, who are encouraged to learn and ask questions, and the Devotary of Denial. Adolin Kholin's calling, for example, is Dueling. The priesthood of the Vorin religion are referred to as ardents.\n",
    "Those who reject the existence of the Almighty, such as Jasnah Kholin, are referred to as heretics. Followers of other religions mentioned in The Way of Kings are Stone Shamans, Ysperists and Maakians.\n",
    "\n",
    "Shardblades and Shardplate:\n",
    "Shardblades are powerful swords that have the ability to cut through any non-living matter with ease. When used on living creatures, they can kill or maim with a single cut by the blade passing through the living soul. They can also render limbs useless when they cut through them. The only known defenses against a Shardblade are Shardplate, shields called \"half-shards\", another Shardblade, or an aluminum blade (according to \"Rhythm of War\"). Those who own a Shardblade can summon their blade from thin air in ten heartbeats and can make their blade disappear at will.[52] The blades are rare and highly valued, and there are estimated to be fewer than one hundred known blades in the world.[53]\n",
    "Shardplate is full plate armor that both protects and strengthens the wearer. The armor provides protection against Surgebinding, as one wearing the armor cannot be \"lashed\" directly.[54] Repeated strikes at the same spot on the armor by regular weapons or Shardblades can cause the armor to crack and break. The armor can be repaired or \"regrown\", though it takes a long time.[55]\n",
    "A full shardbearer, one wielding both Shardblade and Shardplate, is a force capable of turning the tide of battle on its own. Kaladin and Syl express a revulsion to the Shardblades wielded by the Alethi. During Dalinar's visions, he sees the Knights Radiant wearing Shardplate and wielding Shardblades, but he notes that the plate when worn by the Radiants glows. Additionally, the number of Blades and Plate worn by the Radiants is much greater than the number left in the world at the main timeline of The Way of Kings. Most Shardblades are actually dead spren that come alive for a period of time by attuning themselves to their owner's heartbeat.[56]\n",
    "Shardblades wielded by the Knights Radiant are the Knight's spren taking the physical form of a weapon (often a sword, but can take the form of any weapon or a shield). Hence, these Shardblades are a physical manifestation of a living spren. 'Living' Shardblades can be summoned instantly, and do not require the summoning period of ten heartbeats. There are also ten Honorblades that grant the powers of one order of Radiants. These weapons don't appear to be physical manifestations of spren, dead or alive, and were wielded by The Heralds until nine of them were abandoned at the end of Aharietiam, or the last desolation. Szeth, the assassin in white, uses an Honorblade of Jezrien in the first two books, and the Herald, Nalan, wields the Honorblade of the Skybreakers.\n",
    "\n",
    "Magic:\n",
    "Surgebinding\n",
    "Surgebinding refers to a group of ten magic systems that stem from Honor and Cultivation, two of the three Shards of Adonalsium present on Roshar. Each of Surgebinding's ten systems revolves around 'binding' two natural 'Surges,' for instance Gravity and Adhesion, to the Surgebinder's will. Surgebinding is powered by Stormlight, and the ability is granted to humans through bonding with a Spren, a type of elemental spirit native to Roshar. There are ten Surgebinding's branches, with Windrunning and powers of Lightweavers (Transformation - Soulcasting and Illumination - illusions), described most thoroughly.\n",
    "Windrunning is an ability where the wielder uses the power of Stormlight to affect gravity and adhesion. It is described in three methods known as the \"Three Lashings\". A Basic Lashing changes the direction of gravitational pull for an individual (causing the person to be pulled towards another object or direction instead of towards the center of the planet). A Full Lashing is described as creating an almost[57] unbreakable bond between two objects until the Stormlight dissipates. A Reverse Lashing causes an object to have a much stronger gravitational pull, causing other objects to be pulled towards it.[57]\n",
    "The Knights Radiant\n",
    "The Knights Radiant originated through spren copying the abilities which the Heralds obtained through their Honorblades. The Knights Radiant gained power through spren by creating a bond with them called the Nahel bond. The bond gives the spren sentience while giving the human Surgebinding abilities. Two examples are Sylphrena, an Honorspren, who shares a bond with Kaladin, giving him the power to Surgebind; and Pattern, a Liespren (Cryptic), who shares a bond with Shallan, granting her power to Soulcast and create Illusions.\n",
    "The Knights Radiant lived by their order's Five Ideals, called The Immortal Words, with the First Ideal being the same for every order: Life before death, strength before weakness, journey before destination. The other four Ideals are different for each order, with the exception of the Order of the Lightweavers, having only the First Ideal. Lightweavers instead must admit truths to themselves in order to progress. Towards the end of The Way of Kings, Kaladin utters the Second Ideal for the Order of Windrunners: I will protect those who cannot protect themselves. Near the end of Words of Radiance, Kaladin whispers the Third Ideal for the Order of Windrunners: I will protect even those I hate, so long as it is right. At the climax of “Rhythm of War,” he speaks the Fourth Ideal: I accept that there will be those I cannot protect.\n",
    "Orders of the Knights Radiant\n",
    "Windrunners: Manipulate the Surges of Adhesion and Gravitation. Bonded to Honorspren.\n",
    "Skybreakers: Manipulate the Surges of Gravitation and Division. Bonded to Highspren.\n",
    "Dustbringers: Manipulate the Surges of Division and Abrasion. Bonded to Ashspren.\n",
    "Edgedancers: Manipulate the Surges of Abrasion and Progression. Bonded to Cultivationspren.\n",
    "Truthwatchers: Manipulate the Surges of Progression and Illumination. Bonded to Mistspren.\n",
    "Lightweavers: Manipulate the Surges of Illumination and Transformation. Bonded to Liespren (Cryptic).\n",
    "Elsecallers: Manipulate the Surges of Transformation and Transportation. Bonded to Inkspren.\n",
    "Willshapers: Manipulate the Surges of Transportation and Cohesion. Bonded to Lightspren (Reachers).\n",
    "Stonewards: Manipulate the Surges of Cohesion and Tension. Bonded to Peakspren.\n",
    "Bondsmiths: Manipulate the Surges of Tension and Adhesion. Bonded to three unique spren (the Nightwatcher, the Stormfather, and the Sibling). Therefore, there can only be three Bondsmiths.[58]\n",
    "\n",
    "Soulcasting and Shadesmar:\n",
    "Soulcasting is a practice where objects are changed from one form to another. It has proven able to turn rock into smoke, purify blood of poisons, and create food, among many other applications. Soulcasting is done by means of a device called a soulcaster that is powered by gems imbued with Stormlight. The type of gem placed inside the soulcaster determines what the caster can transform. With each use of a soulcaster, there is a chance of the gem cracking and being destroyed, especially when a large amount of matter is changed.[59] The main practitioners of soulcasting are the Ardents of the Vorin religion, however, there are a few exceptions. Shallan's father's steward knew how to use a soulcaster,[60] as he used Shallan's father's soulcaster.\n",
    "By the end of The Way of Kings, Jasnah Kholin and Shallan are capable of doing magic that has very similar effects to Soulcasting but does not require a soulcaster to be used, and does not require that the magic user is in physical contact with the object they transform.[61] This book does not go into great detail, but the magic involves mentally communicating with an unknown source to enter a place called Shadesmar. Shadesmar is described in detail in the book but mostly consists of a world made from tiny glass beads. Once within Shadesmar, the power from a Stormlight-infused gem can be used to manipulate objects.[62]\n",
    "In an interview with Brandon Sanderson, Shadesmar is described as a Cognitive Realm connecting all the worlds in the Cosmere. Sanderson has confirmed that Hoid is very good at using Shadesmar, that this is how Hoid moves between worlds, and that people on other worlds within the Cosmere have ways of accessing Shadesmar which are different from those the characters in this book use.[63]\n",
    "\n",
    "Voidbinding:\n",
    "Similar to Surgebinding, Voidbinding has a collection of surges that the third god of Roshar, Odium, makes available to his selected servants, called Fused. Unlike Knights Radiant, each Fused can only access a single surge, from a list of nine: Gravitation, Division, Abrasion, Progression, Illumination, Transformation, Transportation, Cohesion, and Tension. Adhesion is considered to be exclusive to Honor by the Fused, who cannot access it.\n",
    "Voidbinding includes various other types of magic associated with Odium, such as some of the forms that Parshendi/Singers can take on that align them to Odium's ideals.\n",
    "\n",
    "Old Magic:\n",
    "Very little is known about the Old Magic, a set of powers exclusive to the goddess Cultivation. Its most common effects are those granted by her powerful spren, the Nightwatcher, who will grant boons and curses to supplicants who come to her for assistance; any boon granted is offset by a mandatory applied curse. Supplicants will often find that a boon is not granted as they expected it to, as the Nightwatcher does not always understand human desires and customs; poorly-worded requests can have unsatisfactory results. The curses applied can be anywhere from mild to fully debilitating.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, List, Dict\n",
    "\n",
    "from pydantic import PrivateAttr\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "\n",
    "from langchain_core.embeddings import Embeddings\n",
    "from semantic_router.encoders import BaseEncoder\n",
    "\n",
    "\n",
    "class Embedder(BaseEncoder, Embeddings):\n",
    "    name: str = \"custom-nomic-embedder\"\n",
    "    score_threshold: float = 0.8\n",
    "\n",
    "    embedding_size: int = 768\n",
    "    query_types: Dict = {\n",
    "        \"search_query\": \"Use this when you want to encode a query for question-answering over text that was embedded with search_document.\",\n",
    "        \"search_document\": \"The default embedding task type. Any document you want to use for retrieval or store in a vector database should use this task type.\",\n",
    "        \"classification\": \"Use this if your embeddings are for classification (e.g. training a linear probe for a target classification task)\",\n",
    "        \"clustering\": \"Use this if your embeddings need very high linear separability (e.g. building a topic model on your embeddings)\",\n",
    "    }\n",
    "\n",
    "    _tokenizer: Any = PrivateAttr()\n",
    "    _model: Any = PrivateAttr()\n",
    "\n",
    "    class Config:\n",
    "        arbitrary_types_allowed = True\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        embedding_size: int = 768,\n",
    "        score_threshold=0.8,\n",
    "        name=\"custom-nomic-embedder\",\n",
    "        **data,\n",
    "    ) -> None:\n",
    "        super().__init__(name=name, score_threshold=score_threshold)\n",
    "\n",
    "        self.embedding_size = embedding_size\n",
    "        self.name = name\n",
    "        self.score_threshold = score_threshold\n",
    "\n",
    "        object.__setattr__(\n",
    "            self,\n",
    "            \"_tokenizer\",\n",
    "            AutoTokenizer.from_pretrained(\"bert-base-uncased\", model_max_length=8192),\n",
    "        )\n",
    "        object.__setattr__(\n",
    "            self,\n",
    "            \"_model\",\n",
    "            AutoModel.from_pretrained(\n",
    "                pretrained_model_name_or_path=\"nomic-ai/nomic-embed-text-v1.5\",\n",
    "                trust_remote_code=True,\n",
    "                safe_serialization=True,\n",
    "                rotary_scaling_factor=2,\n",
    "            ),\n",
    "        )\n",
    "        self._model.eval()\n",
    "\n",
    "    def __mean_pooling(self, model_output, attention_mask) -> Tensor:\n",
    "        token_embeddings = model_output[0]\n",
    "        input_mask_expanded = (\n",
    "            attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "        )\n",
    "        return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(\n",
    "            input_mask_expanded.sum(1), min=1e-9\n",
    "        )\n",
    "\n",
    "    def embed(\n",
    "        self,\n",
    "        texts: List[str],\n",
    "        querry: str = \"search_document: \",\n",
    "        return_array=False,\n",
    "        return_ndarray=False,\n",
    "    ) -> Tensor:\n",
    "        \"\"\"Function for creating embeddings\"\"\"\n",
    "\n",
    "        if isinstance(texts, str):\n",
    "            texts = [texts]\n",
    "\n",
    "        texts = [querry + text for text in texts]\n",
    "\n",
    "        encoded_input = self._tokenizer(\n",
    "            texts, padding=True, truncation=True, return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        with torch.no_grad():\n",
    "            model_output = self._model(**encoded_input)\n",
    "\n",
    "        embeddings = self.__mean_pooling(model_output, encoded_input[\"attention_mask\"])\n",
    "        embeddings = F.layer_norm(embeddings, normalized_shape=(embeddings.shape[1],))\n",
    "        embeddings = embeddings[:, : self.embedding_size]\n",
    "        embeddings = F.normalize(embeddings, p=2, dim=1)\n",
    "\n",
    "        if return_ndarray:\n",
    "            return embeddings.numpy()\n",
    "\n",
    "        if return_array:\n",
    "            embedded_list = embeddings.tolist()\n",
    "            if len(embedded_list) == 1:\n",
    "                embedded_list = embedded_list[0]\n",
    "            return embedded_list\n",
    "\n",
    "        return embeddings\n",
    "\n",
    "    def _get_embedding(self, texts: str, querry: str) -> Tensor:\n",
    "        return self.embed(texts=texts, return_array=True)\n",
    "\n",
    "    def embed_documents(self, texts: List[str]) -> List[List[float]]:\n",
    "        \"\"\"Embed search docs. Implemented for LangChain vector store campatability\"\"\"\n",
    "        return self._get_embedding(texts=texts, querry=\"search_document:\")\n",
    "\n",
    "    def embed_query(self, text: str) -> List[float]:\n",
    "        \"\"\"Embed query text.Implemented for LangChain vector store campatability\"\"\"\n",
    "        return self._get_embedding(texts=text, querry=\"search_query:\")\n",
    "\n",
    "    def __call__(\n",
    "        self,\n",
    "        docs: List[str],\n",
    "        batch_size: int = 32,\n",
    "        **kwds: torch.Any,\n",
    "    ) -> List:\n",
    "        \"\"\"__call__ Used for semantic_chunkers\n",
    "\n",
    "        Args:\n",
    "            docs (List[str]): list of strings\n",
    "            batch_size (int, optional): Batch size for embedder. Defaults to 32.\n",
    "\n",
    "        Returns:\n",
    "            List: list of all embeddings\n",
    "        \"\"\"\n",
    "        all_embeddings = []\n",
    "        for i in range(0, len(docs), batch_size):\n",
    "            all_embeddings.extend(self.embed(docs[i : i + batch_size]))\n",
    "\n",
    "        return all_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field, field_validator\n",
    "from typing import List\n",
    "\n",
    "\n",
    "class Question(BaseModel):\n",
    "    question: str = Field(description=\"The quiz question\")\n",
    "    options: List[str] = Field(description=\"List of multiple-choice options\")\n",
    "    correct_answers: List[str] = Field(description=\"List of correct answers\")\n",
    "\n",
    "    # @field_validator(\"options\")\n",
    "    # def check_options_length(cls, v):\n",
    "    #     if len(v) != 4:\n",
    "    #         raise ValueError(\"Each question must have exactly 4 options.\")\n",
    "    #     return v\n",
    "\n",
    "    # @field_validator(\"correct_answers\")\n",
    "    # def check_correct_answers_length(cls, v):\n",
    "    #     if len(v) != 1:\n",
    "    #         raise ValueError(\"There must be exactly one correct answer.\")\n",
    "    #     return v\n",
    "\n",
    "\n",
    "class Quiz(BaseModel):\n",
    "    questions: List[Question] = Field(description=\"List of quiz questions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.output_parsers import PydanticOutputParser\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_core.runnables import RunnableParallel, RunnablePassthrough\n",
    "from semantic_chunkers import StatisticalChunker\n",
    "import asyncio\n",
    "from typing import List\n",
    "\n",
    "\n",
    "class QuizGenerator:\n",
    "    def __init__(self):\n",
    "        self.parser = PydanticOutputParser(pydantic_object=Quiz)\n",
    "        self.prompt_template = PromptTemplate(\n",
    "            template=\"Generate a quiz from the following notes. Use only material from notes.\\nEach question should have four options, and one correct answer which must be included in the options.\\n{format_instructions}\\nNotes:\\n{text}\\n\",\n",
    "            input_variables=[\"text\"],\n",
    "            partial_variables={\n",
    "                \"format_instructions\": self.parser.get_format_instructions()\n",
    "            },\n",
    "        )\n",
    "\n",
    "        self.text_splitter = StatisticalChunker(\n",
    "            encoder=Embedder(),\n",
    "            name=\"statistical_chunker\",\n",
    "            threshold_adjustment=0.01,\n",
    "            dynamic_threshold=True,\n",
    "            window_size=5,\n",
    "            min_split_tokens=100,\n",
    "            max_split_tokens=500,\n",
    "            split_tokens_tolerance=10,\n",
    "            plot_chunks=False,\n",
    "            enable_statistics=False,\n",
    "        )\n",
    "\n",
    "        self.llm = ChatOllama(\n",
    "            model=\"llama3:instruct\",\n",
    "            keep_alive=\"5m\",\n",
    "            temperature=0.2,\n",
    "            mirostat=2,\n",
    "            mirostat_eta=0.1,\n",
    "            mirostat_tau=5,  # Adjust tau for more variability\n",
    "            top_p=0.9,  # Use top-p sampling to control variability\n",
    "            verbose=True,\n",
    "            format=\"json\",\n",
    "        )\n",
    "\n",
    "        self.llm_chain = (\n",
    "            {\"text\": RunnablePassthrough()}\n",
    "            | self.prompt_template\n",
    "            | self.llm\n",
    "            | self.parser\n",
    "        )\n",
    "\n",
    "    def generate_quiz(self, notes: str) -> Quiz:\n",
    "        chunks = self.text_splitter(docs=[notes])[0]\n",
    "        questions = []\n",
    "\n",
    "        for chunk in chunks:\n",
    "            response = self.llm_chain.invoke({\"text\": \" \".join(chunk.splits)})\n",
    "            questions.extend(response.questions)\n",
    "\n",
    "        return Quiz(questions=questions)\n",
    "\n",
    "    async def agenerate_quiz(self, notes: str) -> Quiz:\n",
    "        chunks = self.text_splitter(docs=[notes])[0]\n",
    "        questions = []\n",
    "\n",
    "        async def async_invoke(chunk):\n",
    "            response = await self.llm_chain.ainvoke({\"text\": \" \".join(chunk.splits)})\n",
    "            return response.questions\n",
    "\n",
    "        responses = await asyncio.gather(*[async_invoke(chunk) for chunk in chunks])\n",
    "        questions = [question for response in responses for question in response]\n",
    "\n",
    "        return Quiz(questions=questions)\n",
    "\n",
    "    def generate_quiz_batch(self, notes: str, batch_size: int = 2) -> Quiz:\n",
    "        chunks = self.text_splitter(docs=[notes])[0]\n",
    "        questions = []\n",
    "\n",
    "        for i in range(0, len(chunks), batch_size):\n",
    "            batch_chunks = chunks[i : i + batch_size]\n",
    "            responses = self.llm_chain.batch(\n",
    "                [{\"text\": \" \".join(chunk.splits)} for chunk in batch_chunks],\n",
    "                config={\"max_concurrency\": batch_size},\n",
    "            )\n",
    "            for response in responses:\n",
    "                questions.extend(response.questions)\n",
    "\n",
    "        return Quiz(questions=questions)\n",
    "\n",
    "    async def generate_quiz_abatch(self, notes: str, batch_size: int = 2) -> Quiz:\n",
    "        chunks = self.text_splitter(docs=[notes])[0]\n",
    "        questions = []\n",
    "\n",
    "        async def async_batch_invoke(batch_chunks):\n",
    "            responses = await self.llm_chain.abatch(\n",
    "                [{\"text\": \" \".join(chunk.splits)} for chunk in batch_chunks],\n",
    "                config={\"max_concurrency\": batch_size},\n",
    "            )\n",
    "            return [\n",
    "                question for response in responses for question in response.questions\n",
    "            ]\n",
    "\n",
    "        tasks = [\n",
    "            async_batch_invoke(chunks[i : i + batch_size])\n",
    "            for i in range(0, len(chunks), batch_size)\n",
    "        ]\n",
    "        results = await asyncio.gather(*tasks)\n",
    "        questions = [question for batch in results for question in batch]\n",
    "\n",
    "        return Quiz(questions=questions)\n",
    "\n",
    "    def generate_pquiz(self, notes: str, batch_size: int = 2) -> Quiz:\n",
    "        chunks = self.text_splitter(docs=[notes])[0]\n",
    "        questions = []\n",
    "\n",
    "        for chunk_split in range(0, len(chunks), batch_size):\n",
    "            batch_chunks = chunks[chunk_split : chunk_split + batch_size]\n",
    "            par_chain = RunnableParallel(\n",
    "                {f\"chain_{i}\": self.llm_chain for i in range(len(batch_chunks))}\n",
    "            )\n",
    "\n",
    "            response = par_chain.invoke(\n",
    "                {\n",
    "                    f\"chain_{i}\": {\"text\": \" \".join(chunk.splits)}\n",
    "                    for i, chunk in enumerate(batch_chunks)\n",
    "                }\n",
    "            )\n",
    "\n",
    "            questions.extend(\n",
    "                [response[f\"chain_{i}\"].questions for i in range(len(batch_chunks))]\n",
    "            )\n",
    "\n",
    "        return Quiz(\n",
    "            questions=[\n",
    "                question\n",
    "                for question_sub_array in questions\n",
    "                for question in question_sub_array\n",
    "            ]\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<All keys matched successfully>\n"
     ]
    }
   ],
   "source": [
    "quiz_gen = QuizGenerator()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b42460df5f44c5693fb724509a0555c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(18.60123920440674,\n",
       " Quiz(questions=[Question(question='What is the number of identical layers in the encoder?', options=['1', '2', '6', '3'], correct_answers=['6']), Question(question='What is the first sub-layer in each layer of the encoder?', options=['Position-wise fully connected feed-forward network', 'Multi-head self-attention mechanism', 'Layer normalization', 'Residual connection'], correct_answers=['Multi-head self-attention mechanism']), Question(question='What is the purpose of the residual connections in the encoder?', options=['To facilitate layer normalization', 'To implement multi-head self-attention mechanisms', 'To add complexity to the model', 'To simplify the model'], correct_answers=['To facilitate these residual connections']), Question(question='What is the dimension of the output produced by each sub-layer in the encoder?', options=['256', '512', '1024', '2048'], correct_answers=['512']), Question(question='What is composed of a stack of N = 6 identical layers?', options=['Encoder', 'Decoder', 'Transformer', 'Attention'], correct_answers=['Decoder']), Question(question='What performs multi-head attention over the output of the encoder stack?', options=['Encoder', 'Decoder', 'Self-attention sub-layer', 'Layer normalization'], correct_answers=['Decoder']), Question(question='Why are positions prevented from attending to subsequent positions in the decoder stack?', options=['To prevent self-attention', 'To ensure predictions depend only on known outputs', 'To improve layer normalization', 'To reduce computational complexity'], correct_answers=['To ensure predictions depend only on known outputs'])]))"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = time()\n",
    "quiz_json = quiz_gen.generate_quiz_batch(text_small)\n",
    "time() - t, quiz_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e7bd0e3dbea4801a2b56d42b2150d78",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(19.014702081680298,\n",
       " Quiz(questions=[Question(question='What is the number of identical layers in the encoder?', options=['1', '2', '6', '3'], correct_answers=['6']), Question(question='What is the first sub-layer in each layer of the encoder?', options=['Position-wise fully connected feed-forward network', 'Multi-head self-attention mechanism', 'Layer normalization', 'Residual connection'], correct_answers=['Multi-head self-attention mechanism']), Question(question='What is the second sub-layer in each layer of the encoder?', options=['Simple, position-wise fully connected feed-forward network', 'Multi-head self-attention mechanism', 'Layer normalization', 'Residual connection'], correct_answers=['Simple, position-wise fully connected feed-forward network']), Question(question='What is used to facilitate residual connections in the model?', options=['Sub-layers only', 'Embedding layers only', 'All sub-layers and embedding layers', 'None of the above'], correct_answers=['All sub-layers and embedding layers']), Question(question='What is composed of a stack of N = 6 identical layers?', options=['Encoder', 'Decoder', 'Transformer', 'Attention'], correct_answers=['Decoder']), Question(question='What performs multi-head attention over the output of the encoder stack?', options=['Encoder', 'Decoder', 'Self-Attention', 'Layer Normalization'], correct_answers=['Decoder']), Question(question='Why do we modify the self-attention sub-layer in the decoder stack?', options=['To prevent positions from attending to subsequent positions', 'To enable positions to attend to subsequent positions', 'To add residual connections', 'To apply layer normalization'], correct_answers=['To prevent positions from attending to subsequent positions'])]))"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = time()\n",
    "quiz_json = await quiz_gen.generate_quiz_abatch(text_small)\n",
    "time() - t, quiz_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f386db547e3244e29eff62b69e7078c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(34.21490669250488,\n",
       " Quiz(questions=[Question(question='What is the number of identical layers in the encoder?', options=['1', '6', '3', '9'], correct_answers=['6']), Question(question='What are the two sub-layers in each layer of the encoder?', options=['Self-Attention and Position-wise Fully Connected Feed-forward Network', 'Multi-head Self-Attention and Simple, Position-wise Fully Connected Feed-forward Network', 'Residual Connection and Layer Normalization', 'Encoder and Decoder'], correct_answers=['Multi-head Self-Attention and Simple, Position-wise Fully Connected Feed-forward Network']), Question(question='What is the dimension of the output produced by each sub-layer in the model?', options=['256', '512', '1024', '2048'], correct_answers=['512']), Question(question='What is the purpose of layer normalization in the encoder and decoder?', options=['To facilitate residual connections', 'To prevent positions from attending to subsequent positions', 'To ensure that predictions for position i can depend only on the known outputs at positions less than i', 'To normalize the output of each sub-layer'], correct_answers=['To normalize the output of each sub-layer']), Question(question='What is modified in the self-attention sub-layer in the decoder stack?', options=['The number of identical layers', 'The dimension of the output produced by each sub-layer', 'The masking to prevent positions from attending to subsequent positions', 'The residual connection'], correct_answers=['The masking to prevent positions from attending to subsequent positions']), Question(question='What is the number of identical layers in the encoder?', options=['1', '6', '3', '2'], correct_answers=['6']), Question(question='What are the two sub-layers in each layer of the encoder?', options=['Position-wise fully connected feed-forward network and multi-head self-attention mechanism', 'Simple feed-forward network and position-wise attention mechanism', 'Multi-head self-attention mechanism and simple feed-forward network', 'Residual connection and layer normalization'], correct_answers=['Position-wise fully connected feed-forward network and multi-head self-attention mechanism']), Question(question='What is the function implemented by each sub-layer in the encoder?', options=['LayerNorm(x + Sublayer(x))', 'Sublayer(x)', 'x', 'LayerNorm(x)'], correct_answers=['LayerNorm(x + Sublayer(x))']), Question(question='How many identical layers are in the decoder?', options=['1', '6', '3', '2'], correct_answers=['6']), Question(question='What is the third sub-layer added to each layer of the decoder?', options=['Position-wise fully connected feed-forward network', 'Multi-head attention over the output of the encoder stack', 'Simple feed-forward network', 'Residual connection'], correct_answers=['Multi-head attention over the output of the encoder stack']), Question(question='What prevents positions from attending to subsequent positions in the decoder?', options=['Masking and offsetting the output embeddings', 'Layer normalization', 'Residual connections', 'Self-attention sub-layer'], correct_answers=['Masking and offsetting the output embeddings'])]))"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = time()\n",
    "quiz_json = quiz_gen.generate_pquiz(text_small)\n",
    "time() - t, quiz_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7298a80795914291b9f10ddefa8e2cc7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(22.23336911201477,\n",
       " Quiz(questions=[Question(question='What is the number of identical layers in the encoder?', options=['1', '2', '6', '8'], correct_answers=['6']), Question(question='What is the first sub-layer in each layer of the encoder?', options=['Multi-head self-attention mechanism', 'Simple, position-wise fully connected feed-forward network', 'Layer normalization', 'Residual connection'], correct_answers=['Multi-head self-attention mechanism']), Question(question='What is the second sub-layer in each layer of the encoder?', options=['Multi-head self-attention mechanism', 'Simple, position-wise fully connected feed-forward network', 'Layer normalization', 'Residual connection'], correct_answers=['Simple, position-wise fully connected feed-forward network']), Question(question='What is used to facilitate residual connections in the model and embedding layers?', options=['Sub-layer itself', 'Layer normalization', 'Multi-head self-attention mechanism', 'Residual connection'], correct_answers=['Residual connection']), Question(question='What is the dimension of the output produced by each sub-layer and embedding layer?', options=['128', '256', '512', '1024'], correct_answers=['512']), Question(question='What is composed of a stack of N = 6 identical layers?', options=['Encoder', 'Decoder', 'Transformer', 'Attention'], correct_answers=['Decoder']), Question(question='What performs multi-head attention over the output of the encoder stack?', options=['Encoder', 'Decoder', 'Self-attention sub-layer', 'Layer normalization'], correct_answers=['Decoder']), Question(question='Why do we modify the self-attention sub-layer in the decoder stack?', options=['To prevent positions from attending to subsequent positions', 'To ensure that the predictions for position i can depend only on the known outputs at positions less than i', 'To add residual connections around each of the sub-layers', 'To employ layer normalization'], correct_answers=['To prevent positions from attending to subsequent positions'])]))"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = time()\n",
    "quiz_json = await quiz_gen.agenerate_quiz(text_small)\n",
    "time() - t, quiz_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c74108fc36a34ff9a12956ba69382cbe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(20.545297861099243,\n",
       " Quiz(questions=[Question(question='What is the number of identical layers in the encoder?', options=['1', '2', '6', '3'], correct_answers=['6']), Question(question='What is the first sub-layer in each layer of the encoder?', options=['Position-wise fully connected feed-forward network', 'Multi-head self-attention mechanism', 'Layer normalization', 'Residual connection'], correct_answers=['Multi-head self-attention mechanism']), Question(question='What is the second sub-layer in each layer of the encoder?', options=['Simple, position-wise fully connected feed-forward network', 'Multi-head self-attention mechanism', 'Layer normalization', 'Residual connection'], correct_answers=['Simple, position-wise fully connected feed-forward network']), Question(question='What is used to facilitate residual connections in the model and embedding layers?', options=['Sub-layer itself', 'Layer normalization', 'Residual connection', 'Multi-head self-attention mechanism'], correct_answers=['Sub-layer itself']), Question(question='What is composed of a stack of N = 6 identical layers?', options=['Encoder', 'Decoder', 'Transformer', 'Attention'], correct_answers=['Decoder']), Question(question='What performs multi-head attention over the output of the encoder stack?', options=['Encoder', 'Decoder', 'Self-attention sub-layer', 'Layer normalization'], correct_answers=['Decoder']), Question(question='Why are positions prevented from attending to subsequent positions in the decoder stack?', options=['To prevent position i from depending on unknown outputs at positions greater than i', 'To ensure that predictions for position i can depend only on known outputs at positions less than i', 'To mask the self-attention sub-layer', 'To offset the output embeddings'], correct_answers=['To ensure that predictions for position i can depend only on known outputs at positions less than i'])]))"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = time()\n",
    "quiz_json = quiz_gen.generate_quiz(text_small)\n",
    "time() - t, quiz_json"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "caps",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
